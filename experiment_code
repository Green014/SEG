import os
os.environ["PGCLIENTENCODING"] = "utf-8"

import psycopg2
import time
import random
import json
import matplotlib.pyplot as plt
import numpy as np
import csv  
from faker import Faker

DB_CONFIG = {
    "dbname": "postgres",
    "user": "postgres",
    "password": "631231", 
    "host": "localhost",
    "port": "5432"
}

DATA_SCALES = [100000, 500000] 
REPEAT_COUNT = 3 

fake = Faker()

class DBManager:
    def __init__(self):
        self.conn = psycopg2.connect(**DB_CONFIG)
        self.cur = self.conn.cursor()
        self.conn.autocommit = True

    def run_sql(self, sql, params=None):
        self.cur.execute(sql, params)
    
    def fetch_all_benchmark(self, sql, params=None):
        start = time.perf_counter()
        self.cur.execute(sql, params)
        self.cur.fetchall() 
        return time.perf_counter() - start

    def get_size_metrics(self, table_name):
        self.cur.execute(f"SELECT pg_table_size('{table_name}')")
        t_size = self.cur.fetchone()[0] / 1024 / 1024
        self.cur.execute(f"SELECT pg_indexes_size('{table_name}')")
        i_size = self.cur.fetchone()[0] / 1024 / 1024
        return t_size, i_size

    def close(self):
        self.cur.close()
        self.conn.close()

def reset_tables(db):
    print("   [System] Resetting Schema...")
    tables = ["orders_rel", "users_rel", "user_orders_json_no_index", "user_orders_json_gin"]
    for t in tables: db.run_sql(f"DROP TABLE IF EXISTS {t} CASCADE")

    # Normalized
    db.run_sql("CREATE TABLE users_rel (id SERIAL PRIMARY KEY, name TEXT, email TEXT)")
    db.run_sql("""
        CREATE TABLE orders_rel (
            id SERIAL PRIMARY KEY, user_id INT REFERENCES users_rel(id), 
            amount DECIMAL(10, 2), order_date DATE, status TEXT, product_color TEXT
        )
    """)
    db.run_sql("CREATE INDEX idx_rel_userid ON orders_rel(user_id)")
    db.run_sql("CREATE INDEX idx_rel_color ON orders_rel(product_color)")

    # JSONB
    db.run_sql("CREATE TABLE user_orders_json_no_index (id SERIAL PRIMARY KEY, data JSONB)")
    db.run_sql("CREATE TABLE user_orders_json_gin (id SERIAL PRIMARY KEY, data JSONB)")
    db.run_sql("CREATE INDEX idx_gin ON user_orders_json_gin USING GIN (data)")

def generate_data_in_memory(num_users):
    print(f"   [Data] Generating {num_users} users in RAM...")
    colors_pool = ['red', 'blue', 'green', 'black', 'white', 'yellow', 'purple', 
                   'orange', 'pink', 'grey', 'cyan', 'magenta', 'lime', 'teal', 
                   'indigo', 'violet', 'gold', 'silver', 'brown', 'navy']
    users, orders, json_rows = [], [], []
    for uid in range(1, num_users + 1):
        name, email = fake.name(), fake.email()
        users.append((uid, name, email))
        user_json = {"id": uid, "name": name, "email": email, "orders": []}
        for _ in range(random.randint(1, 5)):
            amount = round(random.uniform(10, 1000), 2)
            date = str(fake.date_this_year())
            status = random.choice(['paid', 'shipped', 'pending'])
            color = random.choice(colors_pool)
            orders.append((uid, amount, date, status, color))
            user_json["orders"].append({
                "amount": amount, "date": date, "status": status, 
                "specs": {"color": color, "weight": "1kg"}
            })
        json_rows.append((json.dumps(user_json),))
    return users, orders, json_rows

def run_experiment():
    db = DBManager()
    
    results = {
        "W1_Ingestion_Rate": {},
        "W1_Table_Size": {}, 
        "W1_Index_Size": {},
        "W2_Point_Lookup": {},
        "W3_Aggregation": {},
        "W4_Filtering": {},
        "W5_Update": {}
    }

    for scale in DATA_SCALES:
        s_label = f"{int(scale/1000)}k"
        print(f"\n========== SCALE: {s_label} ==========")
        for k in results: 
            if s_label not in results[k]: results[k][s_label] = {}

        reset_tables(db)
        users, orders, json_rows = generate_data_in_memory(scale)
        total_rows_rel = len(users) + len(orders)
        total_rows_json = len(json_rows)

        # --- Workload 1: Bulk Ingestion ---
        print(">> Workload 1: Bulk Ingestion...")
        
        # Norm
        start = time.perf_counter()
        args_u = ','.join(db.cur.mogrify("(%s,%s,%s)", x).decode('utf-8') for x in users)
        db.run_sql("INSERT INTO users_rel VALUES " + args_u)
        batch_size = 5000
        for i in range(0, len(orders), batch_size):
            batch = orders[i:i+batch_size]
            args_o = ','.join(db.cur.mogrify("(%s,%s,%s,%s,%s)", x).decode('utf-8') for x in batch)
            db.run_sql("INSERT INTO orders_rel (user_id, amount, order_date, status, product_color) VALUES " + args_o)
        results["W1_Ingestion_Rate"][s_label]["Normalized"] = total_rows_rel / (time.perf_counter() - start)
        t, i_size = db.get_size_metrics("orders_rel")
        results["W1_Table_Size"][s_label]["Normalized"] = t
        results["W1_Index_Size"][s_label]["Normalized"] = i_size

        # JSONB No Index
        start = time.perf_counter()
        for i in range(0, len(json_rows), batch_size):
            batch = json_rows[i:i+batch_size]
            args_j = ','.join(db.cur.mogrify("(%s)", x).decode('utf-8') for x in batch)
            db.run_sql("INSERT INTO user_orders_json_no_index (data) VALUES " + args_j)
        results["W1_Ingestion_Rate"][s_label]["JSONB (No Index)"] = total_rows_json / (time.perf_counter() - start)
        t, i_size = db.get_size_metrics("user_orders_json_no_index")
        results["W1_Table_Size"][s_label]["JSONB (No Index)"] = t
        results["W1_Index_Size"][s_label]["JSONB (No Index)"] = i_size

        # JSONB GIN
        start = time.perf_counter()
        for i in range(0, len(json_rows), batch_size):
            batch = json_rows[i:i+batch_size]
            args_j = ','.join(db.cur.mogrify("(%s)", x).decode('utf-8') for x in batch)
            db.run_sql("INSERT INTO user_orders_json_gin (data) VALUES " + args_j)
        results["W1_Ingestion_Rate"][s_label]["JSONB (GIN)"] = total_rows_json / (time.perf_counter() - start)
        t, i_size = db.get_size_metrics("user_orders_json_gin")
        results["W1_Table_Size"][s_label]["JSONB (GIN)"] = t
        results["W1_Index_Size"][s_label]["JSONB (GIN)"] = i_size

        def measure_avg(func):
            times = [func() for _ in range(REPEAT_COUNT)]
            return sum(times) / len(times) * 1000 # ms

        # --- Workload 2: Point Lookup ---
        print(">> Workload 2: Point Lookup...")
        uid = random.randint(1, scale)
        results["W2_Point_Lookup"][s_label]["Normalized"] = measure_avg(lambda: db.fetch_all_benchmark("SELECT * FROM users_rel u JOIN orders_rel o ON u.id = o.user_id WHERE u.id = %s", (uid,)))
        results["W2_Point_Lookup"][s_label]["JSONB"] = measure_avg(lambda: db.fetch_all_benchmark("SELECT data FROM user_orders_json_no_index WHERE id = %s", (uid,)))

        # --- Workload 3: Aggregation ---
        print(">> Workload 3: Aggregation...")
        results["W3_Aggregation"][s_label]["Normalized"] = measure_avg(lambda: db.fetch_all_benchmark("SELECT SUM(amount) FROM orders_rel"))
        results["W3_Aggregation"][s_label]["JSONB"] = measure_avg(lambda: db.fetch_all_benchmark("SELECT sum((item->>'amount')::numeric) FROM user_orders_json_no_index, jsonb_array_elements(data->'orders') item"))

        # --- Workload 4: Filtering ---
        print(">> Workload 4: Filtering...")
        results["W4_Filtering"][s_label]["Normalized"] = measure_avg(lambda: db.fetch_all_benchmark("SELECT count(*) FROM orders_rel WHERE product_color = 'red'"))
        results["W4_Filtering"][s_label]["JSONB (No Index)"] = measure_avg(lambda: db.fetch_all_benchmark("SELECT count(*) FROM user_orders_json_no_index WHERE data @> '{\"orders\": [{\"specs\": {\"color\": \"red\"}}]}'"))
        results["W4_Filtering"][s_label]["JSONB (GIN)"] = measure_avg(lambda: db.fetch_all_benchmark("SELECT count(*) FROM user_orders_json_gin WHERE data @> '{\"orders\": [{\"specs\": {\"color\": \"red\"}}]}'"))

        # --- Workload 5: Update ---
        print(">> Workload 5: Update...")
        def run_up_norm():
            uid = random.randint(1, scale)
            start = time.perf_counter()
            db.run_sql("""
                UPDATE orders_rel 
                SET status = 'shipped' 
                WHERE id = (
                    SELECT id FROM orders_rel WHERE user_id = %s LIMIT 1
                )
            """, (uid,))
            return time.perf_counter() - start
        def run_up_json(t):
            uid = random.randint(1, scale)
            start = time.perf_counter()
            db.run_sql(f"UPDATE {t} SET data = jsonb_set(data, '{{orders, 0, status}}', '\"shipped\"') WHERE id = %s", (uid,))
            return time.perf_counter() - start
            
        results["W5_Update"][s_label]["Normalized"] = measure_avg(run_up_norm)
        results["W5_Update"][s_label]["JSONB (No Index)"] = measure_avg(lambda: run_up_json("user_orders_json_no_index"))
        results["W5_Update"][s_label]["JSONB (GIN)"] = measure_avg(lambda: run_up_json("user_orders_json_gin"))

    db.close()
    return results

def plot_workload_charts(results):
    print("\n[Plotting] Generating Combined Workload Charts...")
    current_dir = os.path.dirname(os.path.abspath(__file__))
    scales = DATA_SCALES
    scale_labels = [f"{int(s/1000)}k" for s in scales]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    fig.suptitle('Workload 1: Bulk Ingestion Analysis', fontsize=14)

    # Ingestion Rate
    variants = sorted(results["W1_Ingestion_Rate"][scale_labels[0]].keys())
    x = np.arange(len(scale_labels))
    width = 0.2
    for i, var in enumerate(variants):
        vals = [results["W1_Ingestion_Rate"][s][var] for s in scale_labels]
        ax1.bar(x + i*width, vals, width, label=var)
    ax1.set_title("Ingestion Rate (Higher is Better)")
    ax1.set_ylabel("Rows / Second")
    ax1.set_xticks(x + width)
    ax1.set_xticklabels(scale_labels)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Storage Size
    for i, var in enumerate(variants):
        t_vals = np.array([results["W1_Table_Size"][s].get(var, 0) for s in scale_labels])
        i_vals = np.array([results["W1_Index_Size"][s].get(var, 0) for s in scale_labels])
        ax2.bar(x + i*width, t_vals, width, label=f"{var} (Table)")
        ax2.bar(x + i*width, i_vals, width, bottom=t_vals, label=f"{var} (Index)", alpha=0.6, hatch='//')
    ax2.set_title("Storage Footprint (Lower is Better)")
    ax2.set_ylabel("Size (MB)")
    ax2.set_xticks(x + width)
    ax2.set_xticklabels(scale_labels)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(current_dir, "Fig1_Workload_Ingestion_Storage.png"))

    def plot_simple_workload(workload_key, title, filename):
        fig, ax = plt.subplots(figsize=(8, 6))
        data_map = results[workload_key]
        variants = sorted(list(set(v for s in scale_labels for v in data_map[s].keys())))
        x = np.arange(len(scale_labels))
        width = 0.2
        for i, var in enumerate(variants):
            vals = [data_map[s].get(var, 0) for s in scale_labels]
            ax.bar(x + i*width, vals, width, label=var)
        ax.set_title(title)
        ax.set_ylabel("Latency (ms) - Lower is Better")
        ax.set_xticks(x + width * (len(variants)-1)/2)
        ax.set_xticklabels(scale_labels)
        ax.set_xlabel("Data Scale")
        ax.legend()
        ax.grid(True, alpha=0.3)
        plt.savefig(os.path.join(current_dir, filename))
        plt.close()

    plot_simple_workload("W2_Point_Lookup", "Workload 2: Point Lookup Latency", "Fig2_Workload_Lookup.png")
    plot_simple_workload("W3_Aggregation", "Workload 3: Aggregation Latency", "Fig3_Workload_Aggregation.png")
    plot_simple_workload("W4_Filtering", "Workload 4: Filtering Latency", "Fig4_Workload_Filtering.png")
    plot_simple_workload("W5_Update", "Workload 5: Update Operation Latency", "Fig5_Workload_Update.png")
    print("Done plotting charts.")

def save_and_print_results(results):
    print("\n" + "="*40)
    print("      FINAL NUMERICAL RESULTS")
    print("="*40)
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    csv_file = os.path.join(current_dir, "experiment_raw_data.csv")
    
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Workload_Metric', 'Data_Scale', 'Variant', 'Value'])
        
        for metric_name, data_scales in results.items():
            print(f"\n>>> {metric_name}")
            for scale_label, variants in data_scales.items():
                print(f"  [Scale: {scale_label}]")
                for var, val in variants.items():                   
                    print(f"    - {var:<20}: {val:.4f}")              
                    writer.writerow([metric_name, scale_label, var, f"{val:.4f}"])
    
    print("\n" + "="*40)
    print(f"Raw data saved to: {csv_file}")
    print("="*40)

if __name__ == "__main__":
    print("Starting Final Experiment with Data Export...")
    data = run_experiment()
    
    save_and_print_results(data)
    
    plot_workload_charts(data)
    
    print("\nProcess Complete!")
